{"cells":[{"cell_type":"markdown","metadata":{"id":"KwW-E7LXNjUo"},"source":["# Regressão Linear"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install torch torchvision torchaudio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":565},"executionInfo":{"elapsed":14838,"status":"ok","timestamp":1623961474741,"user":{"displayName":"Vitor Casadei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_Qc3i7yEtToSZf9tx_rm0RRTInE8NwNIA5CwhEDM=s64","userId":"15557768782438349168"},"user_tz":180},"id":"ZIsKjZbCfJWK","outputId":"3246941b-cf9e-4544-ff57-64e46d03f3d3"},"outputs":[],"source":["# http://pytorch.org/\n","from os.path import exists\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"_GloDqSbNjUt"},"source":["## Objetivos"]},{"cell_type":"markdown","metadata":{"id":"Hblvoa1WNjUw"},"source":["Os objetivos deste notebook são:\n","\n","* introduzir os conceitos básicos iniciais de redes neurais através da \n","* otimização de um ajuste de pontos por uma reta (linear) utilizando a \n","* técnica de cálculo numérico do gradiente descendente. \n","\n","Este notebook contém uma demonstração iterativa do laço de otimização, com visualização: \n","* reta de ajuste sendo otimizada\n","* valores dos parâmetros sendo ajustados\n","* função de perda sendo minimizada"]},{"cell_type":"markdown","metadata":{"id":"VJl1OjrsNjU4"},"source":["## Problema de Regressão Linear"]},{"cell_type":"markdown","metadata":{"id":"QsQHhxdeNjU5"},"source":["O problema de ajuste de uma reta a um conjunto de pontos para verificar se existe uma previsão linear é um problema muito antigo, muito estudado e muito presente nos dias de hoje. \n","\n","Quando o ajuste é abordado como um problema de **otimização numérica**, ele é a base de boa parte dos **conceitos sobre redes neurais** e iremos explorá-lo aqui como uma forma de introdução às redes neurais. O modelo de regressão linear que iremos utilizar pode\n","ser visto como uma rede neural de apenas uma camada e função de ativação linear."]},{"cell_type":"markdown","metadata":{"id":"qdCaK2ioNjU8"},"source":["## Conjunto de dados: Flores Íris"]},{"cell_type":"markdown","metadata":{"id":"VQqRPnWbNjVA"},"source":["Iremos utilizar duas propriedades do conjunto de dados das flores Íris [Wikipedia-Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set):\n","\n","* o comprimento das sépalas e \n","* o comprimento da pétalas. \n","\n","A ideia será prever o comprimento da pétala, conhecendo-se o comprimento da sépala. Estaremos usando apenas uma propriedade, ou característica ou *feature* do objeto para que seja fácil visualizar o espaço de busca de parâmetros. \n","\n","Vamos utilizar as 50 amostras da variedade versicolor.\n","\n","![alt text](https://raw.githubusercontent.com/vcasadei/images/master/iris_petals_sepals.png)"]},{"cell_type":"markdown","metadata":{"id":"p1kh7I6iNjVC"},"source":["## Dados: leitura e visualização\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-10-17T12:04:55.040276","start_time":"2017-10-17T12:04:54.219705"},"id":"eDxU7OzWNjVF"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import ipywidgets as widgets\n","from IPython import display\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-10-17T12:04:55.061174","start_time":"2017-10-17T12:04:55.042109"},"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":45,"status":"ok","timestamp":1623961475158,"user":{"displayName":"Vitor Casadei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_Qc3i7yEtToSZf9tx_rm0RRTInE8NwNIA5CwhEDM=s64","userId":"15557768782438349168"},"user_tz":180},"id":"EyH2cJXuNjVQ","outputId":"26f06999-74e8-4537-9f58-1355ba1bdf8d"},"outputs":[],"source":["iris = load_iris()\n","data = iris.data[iris.target==1,::2]  # comprimento das sépalas e pétalas, indices 0 e 2\n","\n","x_in = data[:,0:1]\n","y_in = data[:,1:2]\n","iris_pd = pd.DataFrame(x_in, columns=['x_in'])\n","iris_pd['y_in'] = y_in\n","iris_pd.head()"]},{"cell_type":"markdown","metadata":{"id":"opsZfiIWNjVY"},"source":["## Visualização dos dados `x_in` e `y_in` e normalizados"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-10-17T12:04:55.520317","start_time":"2017-10-17T12:04:55.062915"},"colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"elapsed":1239,"status":"ok","timestamp":1623961476357,"user":{"displayName":"Vitor Casadei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_Qc3i7yEtToSZf9tx_rm0RRTInE8NwNIA5CwhEDM=s64","userId":"15557768782438349168"},"user_tz":180},"id":"xOqJILyPNjVZ","outputId":"d9cdba41-edac-4402-9af2-5f89559b3ec6"},"outputs":[],"source":["x = x_in - x_in.min()\n","x /= x.max()    # normalização\n","y = y_in - y_in.min()\n","y /= y.max()\n","\n","fig = plt.figure(figsize=(16,5))\n","ax_in = fig.add_subplot(1,2,1)\n","ax_in.scatter(x_in, y_in)\n","ax_in.set_xlabel('Comprimento sepalas')\n","ax_in.set_ylabel('Comprimento petalas')\n","ax_n = fig.add_subplot(1,2,2)\n","ax_n.scatter(x, y)\n","ax_n.set_xlabel('Comprimento normalizado sepalas')\n","ax_n.set_ylabel('Comprimento normalizado petalas');"]},{"cell_type":"markdown","metadata":{"id":"5zO8hyoQNjVe"},"source":["## Reta de ajuste"]},{"cell_type":"markdown","metadata":{"id":"Gs6Ze3IlNjVf"},"source":["A equação da reta no plano necessita de dois parâmetros, aqui denominados $w_0$ (*bias*) e inclinação $w_1$. Veja figura:\n","\n","![alt text](https://raw.githubusercontent.com/vcasadei/images/master/linhareta.png)\n","\n","A reta de ajuste será dada por:\n","\n","$$ \\hat{y} = w_0 + w_1 x $$\n","\n","onde \n","* $w_1$ é o coeficiente angular da reta e \n","* $w_0$ é a interseção do eixo vertical quando x é igual a zero, também denominado de *bias*.\n","* $x$ é a variável de entrada (comprimento das sépalas) e \n","* $\\hat{y}$ é a predição (comprimento estimado das pétalas)."]},{"cell_type":"markdown","metadata":{"id":"tahiT0nwNjVg"},"source":["## Representação gráfica da equação linear via neurônio"]},{"cell_type":"markdown","metadata":{"id":"Ysht88hZNjVh"},"source":["$ \\hat{y} = 1 w_0 + x_0 w_1 $\n","\n","Temos:\n","- 1 atributo de entrada: $x_0$\n","- 2 parâmetros para serem ajustados (treinados) $w_0$ e $w_1$\n","- 1 classe de saída $\\hat{y}$\n","![alt text]![alt text](https://raw.githubusercontent.com/vcasadei/images/master/RegressaoLinearNeuronio.png)\n","$$ \\hat{y} = w_0 + w_1 x $$\n","$$ \\mathbf{\\hat{y}} = \\mathbf{w} \\mathbf{x} $$"]},{"cell_type":"markdown","metadata":{"id":"z6XeNkvrNjVi"},"source":["### Função Custo ou de Perda (MSE - Mean Square Error)"]},{"cell_type":"markdown","metadata":{"id":"e9_EQADPNjVj"},"source":["![alt text](https://raw.githubusercontent.com/vcasadei/images/master/Loss_MSE.png)\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"cGSG7L-nNjVk"},"source":["A função de custo depende do conjunto de treinamento ($y_i$) e dos valores de predição ($\\hat{y_i}$):\n","\n","$$ J(\\hat{y_i},y_i) = \\frac{1}{M} \\sum_{i=0}^{M-1} (\\hat{y_i} - y_i)^2 $$ .\n"]},{"cell_type":"markdown","metadata":{"id":"WmlOjgV9NjVl"},"source":["## Laço de minimização via gradiente descendente"]},{"cell_type":"markdown","metadata":{"id":"niuRFftZNjVm"},"source":["O código da próxima célula é a parte principal deste notebook. É aqui que a minimização é feita. É aqui que dizemos que estamos fazendo o *fit*, ou o treinamento do sistema para encontrar o parâmetro $\\mathbf{W}$ que minimiza a função de perda $J$. Acompanhamos a convergência da minimização pelo valor da perda a cada iteração, plotando o vetor `J_history`."]},{"cell_type":"markdown","metadata":{"id":"KkyEvBhxNjVm"},"source":["O esquema da otimização é representado pelo diagrama a seguir:\n","![alt text](http://casadei.io/images/RegressaoLinear_Otimizacao.png)\n","\n","e é implementado pela próxima célula de código:"]},{"cell_type":"markdown","metadata":{"id":"Nj5MadtDNjVn"},"source":["## Funções: Custo, Gradiente Descendente"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-10-17T12:04:55.527296","start_time":"2017-10-17T12:04:55.522222"},"id":"rDLj1m58NjVn"},"outputs":[],"source":["def compute_cost(X_b, y, wT):\n","    '''\n","    Compute cost for linear regression\n","    (X,y): amostras rotuladas X(n_samples,2) e y(n_samples,)\n","    wT: vetor coluna de parâmetros (já transposto)\n","       aceita tanto shape (2,1) Para um caso como (2,n_history) para n_history casos\n","    '''\n","    e = X_b.dot(wT) - y\n","    J = (e * e).mean(axis=0)\n","    return J"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-10-17T12:04:55.754548","start_time":"2017-10-17T12:04:55.556216"},"id":"nZrRjl4gNjVr"},"outputs":[],"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.autograd import Variable\n","\n","def train_net(X, y, lr=0.5, n_epochs=50, batch_size=10, shuffle=True, rnd_seed=445):\n","    torch.manual_seed(rnd_seed)\n","    # Data Loader\n","    dloader = DataLoader(TensorDataset(torch.from_numpy(X.astype(np.float32)), \n","                                       torch.from_numpy(y.astype(np.float32))), \n","                         batch_size=batch_size, shuffle=shuffle)\n","    # Modelo da rede\n","    model = torch.nn.Linear(1, 1)\n","    model.load_state_dict(dict(weight=torch.zeros(1,1), bias=torch.zeros(1)))\n","    # Função de perda\n","    criterion = torch.nn.MSELoss()\n","    # Otimizador do gradiente descendente\n","    optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n","    \n","    losses = []\n","    w_history = np.zeros((n_epochs + 1, 2), np.float32)\n","    model.train(True)\n","    # Laço de treinamento - epocas\n","    for epoch in range(n_epochs):\n","        cost = 0\n","        samp = 0\n","        for x, y in dloader:\n","            optimizer.zero_grad()\n","            pred = model(Variable(x))\n","            loss = criterion(pred, Variable(y))\n","            loss.backward()\n","            optimizer.step()\n","            \n","            cost += loss.data\n","            samp += 1\n","            \n","        losses.append(cost / samp)\n","        w, b = [ww.data for ww in model.parameters()]\n","        w_history[epoch + 1, 0] = b[0]\n","        w_history[epoch + 1, 1] = w[0, 0]\n","            \n","    return losses, w_history\n"]},{"cell_type":"markdown","metadata":{"id":"Ut8akzYcNjVx"},"source":["## Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-10-17T12:04:56.009300","start_time":"2017-10-17T12:04:55.756495"},"id":"Ax04Cv7hNjVy"},"outputs":[],"source":["def show_plots(x, y, lr=0.5, n_epochs=50, n_batch=50, shuffle=True, sleep=0.3):\n","    n_samples = y.shape[0]\n","    \n","    # minimização via gradiente descendente\n","    # -------------------------------------    \n","    J_history, w_history = train_net(x, y, lr=lr, n_epochs=n_epochs, batch_size=n_batch, shuffle=shuffle)\n","\n","    # valor ótimo, solução analítica\n","    # ------------------------------\n","    X_bias = np.hstack([np.ones((n_samples, 1)), x])\n","    w_opt = (np.linalg.inv((X_bias.T).dot(X_bias)).dot(X_bias.T)).dot(y)    \n","    X_all = np.linspace(x.min(), x.max(), 100).reshape(100, 1)\n","    X_all_bias = np.hstack([np.ones((100, 1)), X_all])\n","    result_opt = X_all_bias.dot(w_opt)    # Predição do valor ótimo\n","    \n","    # matriz bidimensional de parâmetros\n","    # ----------------------------------\n","    wmin = w_history.min(axis=0)\n","    wmax = w_history.max(axis=0)\n","    D = wmax - wmin\n","    wmin -= D\n","    wmax += D\n","    ww0, ww1 = np.meshgrid(np.linspace(wmin[0], wmax[0], 100), np.linspace(wmin[1], wmax[1], 100))\n","    w_grid = np.c_[ww0.ravel(), ww1.ravel()]\n","    J_grid = compute_cost(X_bias, y, w_grid.T)\n","    J_grid = J_grid.reshape(ww0.shape)\n","    \n","    # Gráficos:\n","    # --------\n","    fig = plt.figure(figsize=(18, 6))\n","    ax_line = fig.add_subplot(1, 3, 1)    # linha reta\n","    ax_grid = fig.add_subplot(1, 3, 2)\n","    ax_loss = fig.add_subplot(1, 3, 3)    # função perda\n","\n","    ax_loss.plot(J_history)\n","    ax_loss.set_title('Perda', fontsize=15)\n","    ax_loss.set_xlabel('epochs', fontsize=10)\n","    ax_loss.set_ylabel('MSE', fontsize=10)\n","\n","    ax_line.scatter(x, y, marker='o', c='b')    # plotagem dos pontos azuis no Iris dataset\n","    ax_line.set_title('Iris dataset', fontsize=15)\n","    ax_line.set_xlabel('x', fontsize=10)\n","    ax_line.set_ylabel('y', fontsize=10)\n","    ax_line.plot(X_all, result_opt, c='r')      # solução analítica, reta vermelha Iris dataset\n","\n","    ax_grid.pcolormesh(ww0, ww1, J_grid, cmap=plt.cm.coolwarm)\n","    ax_grid.contour(ww0, ww1, J_grid, 20)\n","    ax_grid.scatter(w_opt[0], w_opt[1], marker='x', c='w') # Solução analítica\n","    ax_grid.set_title('W', fontsize=15)\n","    ax_grid.set_xlabel('w0', fontsize=10)\n","    ax_grid.set_ylabel('w1', fontsize=10)\n","\n","    # Plot dinâmico\n","    # -------------\n","    n_delta = n_samples // n_batch\n","    ln_v = np.zeros(n_samples, dtype=object)\n","    ln, sc = None, None\n","    for i, w in enumerate(w_history):\n","        if ln: ln.remove()\n","        if sc: sc.remove()\n","        [lnv.remove() for lnv in ln_v if lnv != 0]\n","\n","        y_hat = X_all_bias.dot(w.T)    # Predição via minimização gradiente descendente\n","        w = w.reshape(1, 2)\n","        J = compute_cost(X_bias, y, w.T)\n","        \n","        ax_loss.scatter(i, J)\n","        \n","        ln, = ax_line.plot(X_all, y_hat, c='k', lw=2)    # reta preta iris dataset\n","        \n","        y_pred = X_bias.dot(w.T)\n","        sc = ax_line.scatter(x, y_pred, c='k', marker='x')\n","        for k in range(n_samples):\n","            ln_v[k], = ax_line.plot([x[k], x[k]], [y_pred[k], y[k]], c='y')\n","            \n","        ax_grid.scatter(w_history[i,0], w_history[i,1], c='r', marker='o')\n","        \n","        display.display(fig)\n","        display.clear_output(wait=True)\n","        time.sleep(sleep)\n","        "]},{"cell_type":"markdown","metadata":{"id":"Z9B9gCZ9NjV6"},"source":["## Plotagem iterativa do gradiente descendente, reta ajuste, parâmetros, função perda"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-10-17T12:17:58.064696","start_time":"2017-10-17T12:17:46.936391"},"colab":{"base_uri":"https://localhost:8080/","height":375},"executionInfo":{"elapsed":482,"status":"error","timestamp":1623961476834,"user":{"displayName":"Vitor Casadei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_Qc3i7yEtToSZf9tx_rm0RRTInE8NwNIA5CwhEDM=s64","userId":"15557768782438349168"},"user_tz":180},"id":"dFP__XVtNjV9","outputId":"d0310af5-a7c6-42d7-9d98-dc4ce825b2e9","scrolled":false},"outputs":[],"source":["%matplotlib inline\n","try:\n","    show_plots(x, y, lr=0.6, n_epochs=20, n_batch=25, sleep=0.1)\n","except KeyboardInterrupt:\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"L-sI22uhNjWI"},"source":["![alt text](https://raw.githubusercontent.com/vcasadei/images/master/RegressaoLinear_Otimizacao.png)"]},{"cell_type":"markdown","metadata":{"id":"DHdcMh3oNjWI"},"source":["## Gradiente Descendente Estocástico"]},{"cell_type":"markdown","metadata":{"id":"7cFLX-0lNjWJ"},"source":["![alt text](https://raw.githubusercontent.com/vcasadei/images/master/MinibatchSGD.png)"]},{"cell_type":"markdown","metadata":{"id":"mQIaWQlYNjWJ"},"source":["![alt text](https://raw.githubusercontent.com/vcasadei/images/master/Loss_MSE_SGD.png)"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"GNV7kGWPNjWK"},"source":["## Conclusões desse experimento\n","\n","Quais são as principais conclusões que podemos tirar deste experimento?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ddkh30ADNjWL"},"source":["# Exercícios"]},{"cell_type":"markdown","metadata":{"id":"ldPjMKT9NjWM"},"source":["1. Altere os valores de learning rate e épocas para melhorar o modelo."]},{"cell_type":"markdown","metadata":{"id":"TI7CyYQiAdZA"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cx3zFX9iFfYR"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"name":"2.1-RegressaoLinearDemo.ipynb","provenance":[{"file_id":"https://github.com/vcasadei/curso-deep-learning/blob/master/RegressaoLinearDemo.ipynb","timestamp":1547570428670}]},"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.2"},"vscode":{"interpreter":{"hash":"5fe3e6f0cdaab8afdc61c52912fda83f7c0a71baaea1897dd7498e2df01e69ec"}}},"nbformat":4,"nbformat_minor":0}
